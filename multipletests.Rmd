---
title: "Multiple testing"
author: "Joshua Loftus"
date: "4/2/2020"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

For various technological/historical reasons there is a lot more data in our current era than during the period when classical statistical methods were initially developed.

Simplified descriptions:

* Classical hypothesis testing problem: a **(scientific) theory** derived from previous observations and first principles implies some **experimental hypothesis** $H$ about an observable/measurable quantity $X$. This is formalized by introducing a (low-dimensional) parametric probability model for $X$ and deriving a **statistical (null) hypothesis** $H_0$ about parameters of this distribution, along with the distribution a test statistic $T = T(X)$ would have assuming $H_0$ is true and, ideally, also under some alternative hypothesis $H_A$ (or family of alternatives). Implementation of the testing procedure on real data usually requires solving an optimization problem, possibly with (iterative) numerical algorithms.

-- e.g. Testing the central tendency via a parameter like the mean $\mu = \mathbb E[X]$ and a hypothesis that it equals some pre-specified value, $H_0 : \mu = \mu_0$ vs some other pre-specified value $H_A : \mu = \mu_A$, range of values $\mu > \mu_0$, or union of ranges $\mu \neq \mu_0$.

-- e.g. Testing the difference between two central tendencies $\mu_X = \mathbb E[X], \mu_Y = \mathbb E[Y]$, often a null hypothesis of equality $H_0 : \mu_X - \mu_Y = 0$.

-- e.g. Testing the coefficient in a linear model predicting a pre-specified outcome variable $Y$ using pre-specified predictor variables $X$ combined with some probabilistic assumptions about the errors $\epsilon$ of the linear model $Y = X\beta + \epsilon$.

* A modern hypothesis testing problem: someone has a large dataset (which we assume is available or easily transformed to a rectangular/spreadsheet format, with many variables/columns, many observations/rows, or both). They might have a vague (scientific) theory, but this theory may be related to many separate hypotheses $H^1, H^2, H^3, \ldots$, or one vague hypothesis $H$ about many of the variables in the dataset, and concerning possibly an unknown subset of them. Most likely there is no single, well-motivated probabilistic model for the data. In this setting it is common to **bet on sparsity**: assume there exists some low-dimensional subset/subspace or representation of the data that satisfies some probabilistic modeling assumptions, and then use optimization algorithms to search for this. 

-- e.g. **Simultaneous inference**: we may wish to test statistical hypotheses about each of many variables and provide some kind of statistical guarantee about the combined family of hypotheses, rather than just a separate statistical guarantee for each one individually. For example, we may wish to control the **family-wise error rate** (FWER) which is the probability of making any type 1 errors.

-- e.g. **Selective inference**: select a subset of the hypotheses and provide some statistical guarantee that applies only to the selected subset. For example, we may wish to control the **false-discovery rate** which is the expected proportion of type 1 errors in the selected set.

-- e.g. **High-dimensional (linear) regression**: select a subset of variables to predict a pre-specified outcome variable in a linear model, and provide inferences for the selected predictors using some appropriately modified type of statistical guarantee.

-- e.g. **Machine learning**: a variety of predictive approaches which relax the linear modeling assumption in some way, but may be used in combination with some assumptions that allow interpretable inference about some hypotheses concerning parts of the model.

```{r}
library(tidyverse)
```

## Data generation

Assume
$$
z_i \sim N(\mu_i, 1)
$$
Our goal is to test $H_{0,i} : \mu_i = 0$ against $H_{A,i} : \mu_i < 0$.

```{r}
p <- 1000
threshold <- 1.1*sqrt(2*log(p))
mu <- c(rep(-threshold, 10), rep(0, p - 10))
z <- rnorm(p) + mu
pvalues <- pnorm(z)
```


## Bonferroni-Dunn correction

Control FWER


These are less than alpha iff original p-values are less than alpha/n

```{r}
which(p.adjust(pvalues, method = "bonferroni") < 0.05)
```


## Holm correction

Control FWER

These are less than alpha iff original p-values are less than alpha/n

```{r}
which(p.adjust(pvalues, method = "holm") < 0.05)
```


## Benjamini-Hochberg correction

Control FDR

These are less than alpha iff original p-values are less than alpha/n

```{r}
which(p.adjust(pvalues, method = "BH") < 0.05)
```

## Simulation

### FWER

```{r}
instance <- function(p, sparsity, threshold) {
  mu <- c(rep(-threshold, sparsity), rep(0, p - sparsity))
  z <- rnorm(p) + mu
  pvalues <- pnorm(z)
  adj_pvalues <- p.adjust(pvalues, method = "bonferroni")
  discoveries <- which(adj_pvalues < 0.05)
  true_discoveries <- sum(discoveries <= sparsity)
  false_discoveries <- sum(discoveries > sparsity)
  return(c(true_discoveries, false_discoveries))
}
```

```{r}
mc_sample <- replicate(1000, instance(1000, 10, sqrt(2*log(1000))))
```

```{r}
rowMeans(mc_sample)
```

FWER:

```{r}
mean(mc_sample[2, ] > 0)
```


### FDR

```{r}
instance <- function(p, sparsity, threshold) {
  mu <- c(rep(-threshold, sparsity), rep(0, p - sparsity))
  z <- rnorm(p) + mu
  pvalues <- pnorm(z)
  adj_pvalues <- p.adjust(pvalues, method = "BH")
  discoveries <- which(adj_pvalues < 0.05)
  true_discoveries <- sum(discoveries <= sparsity)
  false_discoveries <- sum(discoveries > sparsity)
  return(c(true_discoveries, false_discoveries))
}
```

```{r}
mc_sample <- replicate(1000, instance(1000, 10, sqrt(2*log(1000))))
```

```{r}
rowMeans(mc_sample)
```


Checking FDR?

```{r}
mean(mc_sample[2, ]/pmax(colSums(mc_sample), 1))
```



## How can we cheat the FDR?

Make the denominator smaller without increasing numerator -- i.e. adding in many true discoveries (known a priori to be true discoveries)

```{r}
p <- 1000
threshold <- 1.1*sqrt(2*log(p))
mu <- c(rep(-threshold, 10), rep(0, p - 10))
z <- rnorm(p) + mu
pvalues <- pnorm(z)
```


Control FDR

These are less than alpha iff original p-values are less than alpha/n

```{r}
pvalues <- c(pvalues, rep(0.00001, 100))
which(p.adjust(pvalues, method = "BH") < 0.05)
```


## Selective inference for marginal screening

```{r}
C <- 2
p <- 10000
Z <- rnorm(p)
selected_Z <- selected_Z <- data.frame(Z = Z[Z > C])
nrow(selected_Z)/p
```

```{r}
mean(selected_Z$Z > qnorm(.95))
```



```{r mysize=TRUE, size='\\footnotesize',}
truncated_Z_pdf <- function(z) dnorm(z)/pnorm(C, lower.tail = F)
# plot code hidden
```

```{r}
maxZ <- max(Z) + .1
ggplot(selected_Z) +
  geom_histogram(bins = 50, aes(x = Z, y = ..density..)) + xlim(0, maxZ) +
  stat_function(fun = truncated_Z_pdf, xlim = c(1, maxZ), linetype  = 2) +
  stat_function(fun = dnorm, linetype  = 1) +
  theme_minimal()
```


Cutoff for significance 

```{r}
pnorm(3.05, lower.tail = FALSE)/pnorm(C, lower.tail = FALSE)
```

Larger than:

```{r}
qnorm(.95)
```

```{r}
mean(selected_Z$Z > 3.05)
```

This controls the **selective type 1 error**


## Power

```{r}
C <- 1
p <- 100
mu <- c(rep(1, 10), rep(0, p - 10))
Z <- rnorm(p) + mu
selection_index <- Z > C
which(selection_index)
```

```{r}
which(Z[selection_index] > qnorm(.95))
```


Cutoff for significance 

```{r}
pnorm(2.41, lower.tail = FALSE)/pnorm(C, lower.tail = FALSE)
```

```{r}
which(Z[selection_index] > 2.41)
```

Testing the non-selected effects to determine if we should do any follow-up on them in future studies


```{r mysize=TRUE, size='\\footnotesize',}
truncated_Z_pdf <- function(z) dnorm(z)/pnorm(C)
# plot code hidden
```

```{r}
unselected_Z <- selected_Z <- data.frame(Z = Z[Z < C])
maxZ <- min(Z) - .1
ggplot(unselected_Z) +
  geom_histogram(bins = 20, aes(x = Z, y = ..density..)) + xlim(maxZ, max(Z) + .1) +
  stat_function(fun = truncated_Z_pdf, xlim = c(maxZ, C), linetype  = 2) +
  stat_function(fun = dnorm, linetype  = 1) +
  theme_minimal()
```

```{r}
pnorm(.84)/pnorm(C)
```

```{r}
which(unselected_Z > .84)
```



## Bonferroni correction after selection


```{r}
C <- 2
p <- 10000
Z <- rnorm(p)
selected_Z <- selected_Z <- data.frame(Z = Z[Z > C])
nrow(selected_Z)/p
```

```{r}
mean(selected_Z$Z > qnorm(.95))
```




